# -*- coding: utf-8 -*-
"""Resume parser.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19d4uGRJCvX4biVGULvmFkQ_LYBKkSzmB
"""

from flask import Flask, request, jsonify
from pdfminer.converter import TextConverter
from pdfminer.pdfinterp import PDFPageInterpreter, PDFResourceManager
from pdfminer.layout import LAParams
from pdfminer.pdfpage import PDFPage
import io
import docx2txt
import re
import spacy
import pandas as pd
from nltk.corpus import stopwords
import os

app = Flask(__name__)

# Load pre-trained model
nlp = spacy.load('en_core_web_sm')

# General stop words
STOPWORDS = set(stopwords.words('english'))

# Education Degrees
EDUCATION = [
    'BE', 'B.E.', 'B.E', 'BS', 'B.S',
    'ME', 'M.E', 'M.E.', 'MS', 'M.S',
    'BTECH', 'B.Tech', 'M.Tech', 'MTECH',
    'SSC', 'HSC', 'CBSE', 'ICSE', 'X', 'XII'
]

def extract_text_from_pdf(pdf_path):
    with open(pdf_path, 'rb') as fh:
        for page in PDFPage.get_pages(fh, caching=True, check_extractable=True):
            resource_manager = PDFResourceManager()
            fake_file_handle = io.StringIO()
            converter = TextConverter(resource_manager, fake_file_handle, codec='utf-8', laparams=LAParams())
            page_interpreter = PDFPageInterpreter(resource_manager, converter)
            page_interpreter.process_page(page)
            text = fake_file_handle.getvalue()
            yield text
            converter.close()
            fake_file_handle.close()

def extract_text_from_doc(doc_path):
    return docx2txt.process(doc_path)

def extract_skills(resume_text):
    nlp_text = nlp(resume_text)
    tokens = [token.text for token in nlp_text if not token.is_stop]
    data = pd.read_csv("skills.csv")
    skills = list(data.columns.values)
    skillset = []
    for token in tokens:
        if token.lower() in skills:
            skillset.append(token)
    for chunk in nlp_text.noun_chunks:
        chunk = chunk.text.lower().strip()
        if chunk in skills:
            skillset.append(chunk)
    return [i.capitalize() for i in set([i.lower() for i in skillset])]

def extract_education(resume_text):
    nlp_text = nlp(resume_text)
    nlp_text = [sent.string.strip() for sent in nlp_text.sents]
    edu = {}
    for index, text in enumerate(nlp_text):
        for tex in text.split():
            tex = re.sub(r'[?|$|.|!|,]', r'', tex)
            if tex.upper() in EDUCATION and tex not in STOPWORDS:
                edu[tex] = text + nlp_text[index + 1]
    education = []
    for key in edu.keys():
        year = re.search(re.compile(r'(((20|19)(\\d{2})))'), edu[key])
        if year:
            education.append((key, ''.join(year[0])))
        else:
            education.append(key)
    return education

@app.route('/parse_resume', methods=['POST'])
def parse_resume():
    file = request.files['file']
    if file.filename.endswith('.pdf'):
        resume_text = ""
        for page in extract_text_from_pdf(file):
            resume_text += ' ' + page
    elif file.filename.endswith('.docx'):
        resume_text = extract_text_from_doc(file)
    else:
        return jsonify({"error": "File format not supported"}), 400
    skills = extract_skills(resume_text)
    education = extract_education(resume_text)
    return jsonify({"skills": skills, "education": education})

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=os.getenv('PORT', 5000))
